{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0vBf9i37y+G4nCwEh5g8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LhsEric/webcrawler/blob/main/Task_09_%E6%8C%87%E5%AE%9A%E9%96%B1%E8%AE%80_%E6%96%87%E6%9C%AC%E8%B3%87%E6%96%99%E8%99%95%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAWX2EXmYw4f",
        "outputId": "4fdacc9d-5034-40b7-9182-6d81e22ade21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fox\n"
          ]
        }
      ],
      "source": [
        "# 正則表達式\n",
        "# 正則表達式是一種強大的文本處理工具，可用於搜索、替換和提取文本。\n",
        "import re\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog.'\n",
        "result = re.search('fox', text)\n",
        "print(result.group(0)) # fox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK 提供了許多自然語言處理的工具和資源。它可以用於分詞、標記、詞性標注和文本分析\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog.'\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens) # ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9SShR7FmPA5",
        "outputId": "d2a1f8ef-dac0-495a-c942-1700a93e4e08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jieba 將中文文本分成詞語，可以使用 Jieba 的 cut 方法。例如：\n",
        "\n",
        "import jieba\n",
        "\n",
        "text = '我喜歡用Python編程'\n",
        "words = jieba.cut(text)\n",
        "print('/'.join(words)) # 我喜/歡用/Python/編程"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEux1oQwsVGP",
        "outputId": "2edf3861-02bc-4fd9-a674-6e84230c19ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我喜/歡用/Python/編程\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jieba 對中文文本進行詞性標注，可以使用 Jieba 的 posseg 方法\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "text = '我喜歡用Python編程'\n",
        "words = pseg.cut(text)\n",
        "for word, pos in words:\n",
        "    print(word, pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnU4PrTHt9QP",
        "outputId": "94c2fc73-e81d-44db-806c-b2f46deb46fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我 r\n",
            "喜歡 v\n",
            "用 p\n",
            "Python eng\n",
            "編程 n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取中文文本的關鍵詞，可以使用 Jieba 的 extract_tags 方法\n",
        "import jieba.analyse\n",
        "\n",
        "text = '我喜歡用Python編程'\n",
        "keywords = jieba.analyse.extract_tags(text, topK=3)\n",
        "print(keywords) # ['Python', '喜歡', '編程']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udkHb9HxuEel",
        "outputId": "b1d3248b-c1c7-44f6-e1e2-2929f7996f34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['我喜', '歡用', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自然語言處理"
      ],
      "metadata": {
        "id": "vHQLNGKYv4-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob 是一個基於 NLTK 的 Python 库，提供了一個簡單的 API，用於文本分析、情感分析和主題分析。例如：\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = 'I love Python!'\n",
        "blob = TextBlob(text)\n",
        "print(blob.sentiment.polarity) # 0.5 情感分析分數"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFMOIzSzv6iR",
        "outputId": "a5b262b0-2c2d-4266-bb89-eb7631a1e8af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gensim 是一個用於主題建模和相似度比較的 Python 函式庫。它可以用於建立文檔、詞彙和主題模型，以及進行文本相似度比較。例如：\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "    \"I love Python and machine learning\"\n",
        "]\n",
        "texts = [[word for word in document.lower().split()] for document in documents]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
        "query = \"Python and machine learning\"\n",
        "query_bow = dictionary.doc2bow(query.lower().split())\n",
        "sims = index[tfidf[query_bow]]\n",
        "print(list(enumerate(sims)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx-GxotYwV5Z",
        "outputId": "73d3a049-925d-4e21-f048-65fd11a78f2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.0), (1, 0.024158917), (2, 0.78141415)]\n"
          ]
        }
      ]
    }
  ]
}